\documentclass[a4paper]{article}
\usepackage[left=20mm, right=20mm, top=20mm, bottom=20mm]{geometry}
\usepackage{amsthm, amsmath, amssymb}
\usepackage[framemethod=TikZ]{mdframed}

\newenvironment{exercise}[1][]{%
  \mdfsetup{frametitle={%
    \tikz[baseline=(current bounding box.east),outer sep=0pt]
    \node[anchor=east,rectangle,fill=blue!20]
    {\strut Exercise~#1};}}%
  \mdfsetup{
    innertopmargin=3pt,linecolor=blue!20,%
    linewidth=2pt,topline=true,%
    frametitleaboveskip=\dimexpr-\ht\strutbox\relax}
  \begin{mdframed}[]\relax}
  {\end{mdframed}}

\newenvironment{note}[1][]{%
  \mdfsetup{frametitle={%
    \tikz[baseline=(current bounding box.east),outer sep=0pt]
    \node[anchor=east,rectangle,fill=red!40]
    {\strut Remarks~(#1)};}}
  \mdfsetup{
    innertopmargin=3pt,linecolor=red!40,%
    linewidth=2pt,topline=true,%
    frametitleaboveskip=\dimexpr-\ht\strutbox\relax}
  \begin{mdframed}[]\relax}
  {\end{mdframed}}

\title{Notes on Chapter 2}
\author{Chan Kha Vu}
\date{}

\begin{document}
\maketitle

\begin{exercise}[2.4]
  The estimated rewards are defined as $Q_t(a) \doteq c_n R_1 + c_{n-1} R_2 + \ldots + c_1 R_n$,
  where in this case $c_i = \Pi_{j=1}^i \alpha_j\,.$
\end{exercise}

\begin{exercise}[2.6]
  \noindent

  Let's break it down to several intuitive stages of the learning process.
  First, all estimations $Q_0(a)$ are larger than their true value $q^*(a)$,
  so the agent will go through every of the bandits, till the $k$-th step.
  After that, the value estimated at $\arg\max q(a)$ is likely to be the
  largest, that explains why we have a huge spike.

  Now we need to explain the drop after the spike. Remember that the estimations
  $Q_t(a)$ are calculated as the mean over time $t$. Because $Q_0(a^*)$ was very
  large, the estimations over time gets closer to its real value, whilst the
  estimations on other bandits are larger (since they were only estimated once).

  So, our agent comes back to try out the other values of $a$, only to find that
  they are less than the optimal bandit $a^*$. As we can see from the graph, it
  gradually returns to the optimal one.
\end{exercise}

\begin{note}[page 30]
  \noindent

  The claim that \(\sum_a \frac{\partial \pi(a)}{\partial H_t(b)} = 0\), where $\pi_t(a)$ is softmax function over arguments $H_t(b)$, is not obvious at all.
  So, let's prove it.

\begin{align*}
  \frac{\partial \pi_t(a)}{\partial H_t(b)}
  &\doteq \frac{\partial }{\partial H_t(b)} \frac{e^{H_t(a)}}{\sum_i e^{H_t(i)}}
  \tag{$\frac{\partial fg}{\partial x} = \frac{\partial f}{\partial x} g + \frac{\partial g}{\partial x} f$}
  \\ &=
  \left( \frac{\partial}{\partial H_t(b)} e^{H_t(a)} \right) \cdot
  \frac{1}{\sum_i e^{H_t(i)}}
  +
  \left( \frac{\partial}{\partial H_t(b)} \frac{1}{\sum_i e^{H_t(i)}} \right) \cdot
  e^{H_t(a)}
  \tag{$\frac{\partial }{\partial x} \frac{1}{f} = -\frac{1}{f^2} \frac{\partial f}{\partial x} $}
  \\ &=
  \left( \boldsymbol{1}_{a=b} \cdot e^{H_t(a)} \right) \cdot \frac{1}{\sum_i e^{H_t(i)}}
  -
  \frac{e^{H_t(b)}}{\left(\sum_i e^{H_t(i)}\right)^2} \cdot
  e^{H_t(a)}
  \\ &=
  \frac{\boldsymbol{1}_{a=b} \cdot \sum_i e^{H_t(i)} e^{H_t(a)} - e^{H_t(b)} e^{H_t(a)}}{\left(\sum_i e^{H_t(i)}\right)^2 }
  \tag{$\sum_i e^{H_t(i)}$ is constant}
\end{align*}

Now, let's use the fact that $\pi_t(a)$ sums to one, because it is a softmax function. Now, the claim becomes pretty transparent:

\begin{align*}
  \sum_a \frac{\partial \pi_t(a)}{\partial H_t(b)}
  &=
  \sum_a \frac{\boldsymbol{1}_{a=b} \cdot \sum_i e^{H_t(i)} e^{H_t(a)} - e^{H_t(b)} e^{H_t(a)}}{\left(\sum_i e^{H_t(i)}\right)^2 }
  \\ &=
  \frac{\sum_i e^{H_t(i)} e^H_t(b)}{\left( \sum_i e^{H_t(i)} \right)^2} -
  \frac{\sum_a e^{H_t(b)} e^{H_t(a)}}{\left( \sum_i e^{H_t(i)} \right)^2}
  = 0
  \tag{Q.E.D.}
\end{align*}
\end{note}


\end{document}
